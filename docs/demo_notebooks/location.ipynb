{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worse-jersey",
   "metadata": {},
   "source": [
    "# Demo notebook for analysing location data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-intervention",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "GPS location data contain rich information about people's behavioral and mobility patterns. However, working with such data is a challenging task since there exists a lot of noise and missingness. Also, designing relevant features to gain knowledge about the mobility pattern of subjects is a crucial task. To address these problems, `niimpy` provides these main functions to clean, downsample, and extract features from GPS location data:\n",
    "\n",
    "- `niimpy.preprocessing.location.filter_location`: removes low-quality location data points\n",
    "- `niimpy.util.aggregate`: downsamples data points to reduce noise\n",
    "- `niimpy.preprocessing.location.extract_features_location`: feature extraction from location data\n",
    "\n",
    "In the following, we go through analysing a subset of location data provided in [StudentLife](https://studentlife.cs.dartmouth.edu/dataset.html) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-evanescence",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import niimpy\n",
    "import niimpy.preprocessing.location as nilo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = niimpy.read_csv(niimpy.sampledata.LOCATION_FILE, tz='et')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-interpretation",
   "metadata": {},
   "source": [
    "The necessary columns for further analysis are `double_latitude`, `double_longitude`, `double_speed`, and `user`. `user` refers to a unique identifier for a subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-outside",
   "metadata": {},
   "source": [
    "## Filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-reader",
   "metadata": {},
   "source": [
    "Three different methods for filtering low-quality data points are implemented in `niimpy`:\n",
    "\n",
    "- `remove_disabled`: removes data points whose `disabled` column is `True`.\n",
    "- `remove_network`: removes data points whose `provider` column is `network`. This method keeps only `gps`-derived data points.\n",
    "- `remove_zeros`: removes data points close to the point \\<lat=0, lon=0\\>.\n",
    "\n",
    "There is no such data points in this dataset; therefore the dataset does not change after this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nilo.filter_location(data, remove_disabled=False, remove_network=False, remove_zeros=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-october",
   "metadata": {},
   "source": [
    "## Downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-angle",
   "metadata": {},
   "source": [
    "Because GPS records are not always very accurate and they have random errors, it is a good practice to downsample or aggregate data points which are recorded in close time windows. In other words, all the records in the same time window are aggregated to form one GPS record associated to that time window. There are a few parameters to adjust the aggregation setting:\n",
    "\n",
    "- `freq`: represents the length of time window. This parameter follows the formatting of `freq` parameter in pandas resample function. For example '5T' means 5 minute intervals.\n",
    "- `method_numerical`: specifies how numerical columns should be aggregated. Options are 'mean', 'median', 'sum'.\n",
    "- `mthod_categorical`: specifies how categorical columns should be aggregated. Options are 'first', 'mode' (most frequent), 'last'.\n",
    "\n",
    "The aggregation is performed for each `user` (subject) separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_data = niimpy.util.aggregate(data, freq='5T', method_numerical='median')\n",
    "binned_data = binned_data.reset_index(0).dropna()\n",
    "binned_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-enforcement",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-barbados",
   "metadata": {},
   "source": [
    "Here is the list of features `niimpy` extracts from location data:\n",
    "\n",
    "1. distance based features (`niimpy.preprocessing.location.location_distance_features`):\n",
    " - `dist_total`: total distance a person traveled in meter.\n",
    " - `variance`, `log_variance`: variance is defined as sum of variance in latitudes and longitudes.\n",
    " - `speed_average`, `speed_variance`, and `speed_max`: statistics of speed (m/s). Speed, if not given, can be calculated by dividing the distance between two consequitive bins by their time difference.\n",
    " - `n_bins`: number of location bins that a user recorded in dataset.\n",
    "\n",
    "2. Significant Place related features (`niimpy.preprocessing.location.location_significant_place_features`):\n",
    " - `n_static`: number of static points. Static points are defined as bins whose speed is lower than a threshold.\n",
    " - `n_moving`: number of moving points. Equivalent to `n_bins - n_static`.\n",
    " - `n_home`: number of static bins which are close to the person's home. Home is defined the place most visited during nights. More formally, all the locations recorded during 12 Am and 6 AM are clusterd and the center of largest cluster is assumed to be home.\n",
    " - `max_dist_home`: maximum distance from home.\n",
    " - `n_sps`: number of significant places. All of the static bins are clusterd using DBSCAN algorithm. Each cluster represents a Signicant Place (SP) for a user.\n",
    " - `n_rare`: number of rarely visited (referred as outliers in DBSCAN).\n",
    " - `n_transitions`: number of transitions between significant places.\n",
    " - `n_top1`, `n_top2`, `n_top3`, `n_top4`, `n_top5`: number of bins in the top `N` cluster. In other words, `n_top1` shows the number of times the person has visited the most freqently visited place.\n",
    " - `entropy`, `normalized_entropy`: entropy of time spent in clusters. Normalized entropy is the entropy divided by the number of clusters.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# extract all the available features\n",
    "all_features = nilo.extract_features_location(binned_data)\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only distance realted featuers\n",
    "distance_features = nilo.extract_features_location(\n",
    "    binned_data,\n",
    "    feature_functions=[nilo.location_distance_features])\n",
    "distance_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-retailer",
   "metadata": {},
   "source": [
    "## Implementing your own features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-nightmare",
   "metadata": {},
   "source": [
    "If you want to implement a customized feature you can do so with defining a function that accepts a dataframe and returns a dataframe or a series. The reterned object should be indexed by `user`s. Then, when calling `extract_features_location` function, you add the newly implemented function to the `feature_functions` argument. The default feature functions implemented in `niimpy` are in this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "nilo.ALL_FEATURE_FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-feelings",
   "metadata": {},
   "source": [
    "You can add your new function to the `nilo.ALL_FEATURE_FUNCTIONS` list and call `extract_features_location` function. Or if you are interested in only extracting your desired feature you can pass a list containing just that function, like here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized function. It is necessary to add **kwargs term in the function definition.\n",
    "def max_speed(df, **kwargs):\n",
    "    grouped = df.groupby('user')\n",
    "    return grouped['double_speed'].max()\n",
    "\n",
    "customized_features = nilo.extract_features_location(\n",
    "    binned_data,\n",
    "    feature_functions=[max_speed]\n",
    ")\n",
    "customized_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
